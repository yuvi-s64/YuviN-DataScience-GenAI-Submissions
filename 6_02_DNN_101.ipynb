{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/yuvi-s64/YuviN-DataScience-GenAI-Submissions/blob/main/6_02_DNN_101.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "![](https://drive.google.com/uc?export=view&id=1xqQczl0FG-qtNA2_WQYuWePW9oU8irqJ)"
      ],
      "metadata": {
        "id": "E0T9_-jFXxxc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6.02 Dense Neural Network (with PyTorch)\n",
        "This will expand on our logistic regression example and take us through building our first neural network. If you haven't already, be sure to check (and if neccessary) switch to GPU processing by clicking Runtime > Change runtime type and selecting GPU. We can test this has worked with the following code:"
      ],
      "metadata": {
        "id": "dcEWDwlu94Xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Check for GPU availability\n",
        "print(\"Num GPUs Available: \", torch.cuda.device_count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v8cIpNbCvuQA",
        "outputId": "1e3982a9-afdb-452b-ee76-62b5bcbef8b5"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Hopefully your code shows you have 1 GPU available! Next let's get some data. We'll start with another in-built dataset:"
      ],
      "metadata": {
        "id": "8d6FF1wK-ph8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# upload an in-built Python (OK semi-in-built) dataset\n",
        "from sklearn.datasets import load_diabetes\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# import the data\n",
        "data = load_diabetes()\n",
        "data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2MziWWXu-0ur",
        "outputId": "510593c1-0846-4572-bf1f-df78c62e3fae"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'data': array([[ 0.03807591,  0.05068012,  0.06169621, ..., -0.00259226,\n",
              "          0.01990749, -0.01764613],\n",
              "        [-0.00188202, -0.04464164, -0.05147406, ..., -0.03949338,\n",
              "         -0.06833155, -0.09220405],\n",
              "        [ 0.08529891,  0.05068012,  0.04445121, ..., -0.00259226,\n",
              "          0.00286131, -0.02593034],\n",
              "        ...,\n",
              "        [ 0.04170844,  0.05068012, -0.01590626, ..., -0.01107952,\n",
              "         -0.04688253,  0.01549073],\n",
              "        [-0.04547248, -0.04464164,  0.03906215, ...,  0.02655962,\n",
              "          0.04452873, -0.02593034],\n",
              "        [-0.04547248, -0.04464164, -0.0730303 , ..., -0.03949338,\n",
              "         -0.00422151,  0.00306441]]),\n",
              " 'target': array([151.,  75., 141., 206., 135.,  97., 138.,  63., 110., 310., 101.,\n",
              "         69., 179., 185., 118., 171., 166., 144.,  97., 168.,  68.,  49.,\n",
              "         68., 245., 184., 202., 137.,  85., 131., 283., 129.,  59., 341.,\n",
              "         87.,  65., 102., 265., 276., 252.,  90., 100.,  55.,  61.,  92.,\n",
              "        259.,  53., 190., 142.,  75., 142., 155., 225.,  59., 104., 182.,\n",
              "        128.,  52.,  37., 170., 170.,  61., 144.,  52., 128.,  71., 163.,\n",
              "        150.,  97., 160., 178.,  48., 270., 202., 111.,  85.,  42., 170.,\n",
              "        200., 252., 113., 143.,  51.,  52., 210.,  65., 141.,  55., 134.,\n",
              "         42., 111.,  98., 164.,  48.,  96.,  90., 162., 150., 279.,  92.,\n",
              "         83., 128., 102., 302., 198.,  95.,  53., 134., 144., 232.,  81.,\n",
              "        104.,  59., 246., 297., 258., 229., 275., 281., 179., 200., 200.,\n",
              "        173., 180.,  84., 121., 161.,  99., 109., 115., 268., 274., 158.,\n",
              "        107.,  83., 103., 272.,  85., 280., 336., 281., 118., 317., 235.,\n",
              "         60., 174., 259., 178., 128.,  96., 126., 288.,  88., 292.,  71.,\n",
              "        197., 186.,  25.,  84.,  96., 195.,  53., 217., 172., 131., 214.,\n",
              "         59.,  70., 220., 268., 152.,  47.,  74., 295., 101., 151., 127.,\n",
              "        237., 225.,  81., 151., 107.,  64., 138., 185., 265., 101., 137.,\n",
              "        143., 141.,  79., 292., 178.,  91., 116.,  86., 122.,  72., 129.,\n",
              "        142.,  90., 158.,  39., 196., 222., 277.,  99., 196., 202., 155.,\n",
              "         77., 191.,  70.,  73.,  49.,  65., 263., 248., 296., 214., 185.,\n",
              "         78.,  93., 252., 150.,  77., 208.,  77., 108., 160.,  53., 220.,\n",
              "        154., 259.,  90., 246., 124.,  67.,  72., 257., 262., 275., 177.,\n",
              "         71.,  47., 187., 125.,  78.,  51., 258., 215., 303., 243.,  91.,\n",
              "        150., 310., 153., 346.,  63.,  89.,  50.,  39., 103., 308., 116.,\n",
              "        145.,  74.,  45., 115., 264.,  87., 202., 127., 182., 241.,  66.,\n",
              "         94., 283.,  64., 102., 200., 265.,  94., 230., 181., 156., 233.,\n",
              "         60., 219.,  80.,  68., 332., 248.,  84., 200.,  55.,  85.,  89.,\n",
              "         31., 129.,  83., 275.,  65., 198., 236., 253., 124.,  44., 172.,\n",
              "        114., 142., 109., 180., 144., 163., 147.,  97., 220., 190., 109.,\n",
              "        191., 122., 230., 242., 248., 249., 192., 131., 237.,  78., 135.,\n",
              "        244., 199., 270., 164.,  72.,  96., 306.,  91., 214.,  95., 216.,\n",
              "        263., 178., 113., 200., 139., 139.,  88., 148.,  88., 243.,  71.,\n",
              "         77., 109., 272.,  60.,  54., 221.,  90., 311., 281., 182., 321.,\n",
              "         58., 262., 206., 233., 242., 123., 167.,  63., 197.,  71., 168.,\n",
              "        140., 217., 121., 235., 245.,  40.,  52., 104., 132.,  88.,  69.,\n",
              "        219.,  72., 201., 110.,  51., 277.,  63., 118.,  69., 273., 258.,\n",
              "         43., 198., 242., 232., 175.,  93., 168., 275., 293., 281.,  72.,\n",
              "        140., 189., 181., 209., 136., 261., 113., 131., 174., 257.,  55.,\n",
              "         84.,  42., 146., 212., 233.,  91., 111., 152., 120.,  67., 310.,\n",
              "         94., 183.,  66., 173.,  72.,  49.,  64.,  48., 178., 104., 132.,\n",
              "        220.,  57.]),\n",
              " 'frame': None,\n",
              " 'DESCR': '.. _diabetes_dataset:\\n\\nDiabetes dataset\\n----------------\\n\\nTen baseline variables, age, sex, body mass index, average blood\\npressure, and six blood serum measurements were obtained for each of n =\\n442 diabetes patients, as well as the response of interest, a\\nquantitative measure of disease progression one year after baseline.\\n\\n**Data Set Characteristics:**\\n\\n:Number of Instances: 442\\n\\n:Number of Attributes: First 10 columns are numeric predictive values\\n\\n:Target: Column 11 is a quantitative measure of disease progression one year after baseline\\n\\n:Attribute Information:\\n    - age     age in years\\n    - sex\\n    - bmi     body mass index\\n    - bp      average blood pressure\\n    - s1      tc, total serum cholesterol\\n    - s2      ldl, low-density lipoproteins\\n    - s3      hdl, high-density lipoproteins\\n    - s4      tch, total cholesterol / HDL\\n    - s5      ltg, possibly log of serum triglycerides level\\n    - s6      glu, blood sugar level\\n\\nNote: Each of these 10 feature variables have been mean centered and scaled by the standard deviation times the square root of `n_samples` (i.e. the sum of squares of each column totals 1).\\n\\nSource URL:\\nhttps://www4.stat.ncsu.edu/~boos/var.select/diabetes.html\\n\\nFor more information see:\\nBradley Efron, Trevor Hastie, Iain Johnstone and Robert Tibshirani (2004) \"Least Angle Regression,\" Annals of Statistics (with discussion), 407-499.\\n(https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf)\\n',\n",
              " 'feature_names': ['age',\n",
              "  'sex',\n",
              "  'bmi',\n",
              "  'bp',\n",
              "  's1',\n",
              "  's2',\n",
              "  's3',\n",
              "  's4',\n",
              "  's5',\n",
              "  's6'],\n",
              " 'data_filename': 'diabetes_data_raw.csv.gz',\n",
              " 'target_filename': 'diabetes_target.csv.gz',\n",
              " 'data_module': 'sklearn.datasets.data'}"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are working on a regression problem, with \"structured\" data which has already been cleaned and normalised. We can skip the usual cleaning/engineering steps. However, we do need to get the data into PyTorch:"
      ],
      "metadata": {
        "id": "cZKrbx70_cIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert data to PyTorch tensors\n",
        "X = torch.tensor(data.data, dtype=torch.float32)\n",
        "y = torch.tensor(data.target, dtype=torch.float32).reshape(-1, 1) # Reshape y to be a column vector"
      ],
      "metadata": {
        "id": "f9PHiljr73fI"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our data is stored in tensors we can do train/test splitting as before (in fact we can use sklearn as before):"
      ],
      "metadata": {
        "id": "hu8VH2_SAOoj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "print(X_train.shape, y_train.shape)\n",
        "print(X_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYJN01DV8Fac",
        "outputId": "a58a6ba3-950f-4c51-dec2-d5a16baee21c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([353, 10]) torch.Size([353, 1])\n",
            "torch.Size([89, 10]) torch.Size([89, 1])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can set up our batches for training. As we have a nice round 400 let's go with batches of 50 (8 batches in total). We'll also seperate the features and labels:"
      ],
      "metadata": {
        "id": "LKmbZoCrJijU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "# Create TensorDatasets and DataLoaders\n",
        "train_dataset = TensorDataset(X_train, y_train)\n",
        "train_loader = DataLoader(train_dataset, batch_size=50, shuffle=True)\n",
        "\n",
        "test_dataset = TensorDataset(X_test, y_test)\n",
        "test_loader = DataLoader(test_dataset, batch_size=50, shuffle=False)"
      ],
      "metadata": {
        "id": "de0uOko08d-K"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now its time to build our model. We'll keep it simple ... a model with an input layer of 10 features and then 2x _Dense_ (fully connected) layers each with 5 neurons and ReLU activation. Our output layer will be size=1 given this is a regression problem and we want a single value output per prediction.\n",
        "\n",
        "This will be easier to understand if you have read through the logistic regression tutorial."
      ],
      "metadata": {
        "id": "yCCG8kKHCVnd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Define the model\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        # we'll set up the layers as a sequence using nn.Sequential\n",
        "        self.layers = nn.Sequential(\n",
        "\n",
        "            # first layer will be a linear layer that has 5x neurons\n",
        "            # (5x sets of linear regression)\n",
        "            # the layer takes the 10 features as input (i.e. 10, 5)\n",
        "            nn.Linear(10, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU activation\n",
        "\n",
        "            # second linear layer again has 5 neurons\n",
        "            # this time taking the input as the output of the last layer\n",
        "            # (which had 5x neurons)\n",
        "            nn.Linear(5, 5),\n",
        "\n",
        "            nn.ReLU(), # ReLU again\n",
        "\n",
        "            # last linear layer takes the output from the previous 5 neurons\n",
        "            # this time its a single output with no activation\n",
        "            # i.e. this is the predicitons (regression)\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x) # pass the data through the layers"
      ],
      "metadata": {
        "id": "844H60hcCV3s"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before we need to create a model object, specify the loss (criterion) and an optimiser (which we cover next week):"
      ],
      "metadata": {
        "id": "cv4-loCz91aa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim as optim\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss() # MSE loss function\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "EPx_Wy6g9uA4"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can train the model. Again, the logistic regression tutorial (6.01) may help you undertstand this:"
      ],
      "metadata": {
        "id": "HOKfjkfW-Ish"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training loop (example - you'll likely want to add more epochs)\n",
        "epochs = 100 # 100 epochs\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  # use the train_loader to pass the inputs (x) and targets (y)\n",
        "  for inputs, targets in train_loader:\n",
        "    # pass to the GPU (hopefully)\n",
        "    inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "    # pass model to GPU as well\n",
        "    model.to(device)\n",
        "\n",
        "    model.train() # put the model object in train mode\n",
        "    optimiser.zero_grad() # reset the gradiants\n",
        "    outputs = model(inputs) # create outputs\n",
        "    loss = criterion(outputs, targets) # compare with Y to get loss\n",
        "    loss.backward() # backpropogate the loss (next week)\n",
        "    optimiser.step() # # update the parameters based on this round of training\n",
        "\n",
        "  # every 10 steps we will print out the current loss\n",
        "    if (epoch+1) % 10 == 0: # modular arithmetic\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtMUgfwT-HGt",
        "outputId": "79b4bc75-32d8-457a-f890-2fec17346ee9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [10/100], Loss: 35465.5273\n",
            "Epoch [10/100], Loss: 25143.334\n",
            "Epoch [10/100], Loss: 30627.375\n",
            "Epoch [10/100], Loss: 27042.3672\n",
            "Epoch [10/100], Loss: 19174.418\n",
            "Epoch [10/100], Loss: 34107.9648\n",
            "Epoch [10/100], Loss: 36817.3828\n",
            "Epoch [10/100], Loss: 27329.0547\n",
            "Epoch [20/100], Loss: 30353.416\n",
            "Epoch [20/100], Loss: 25781.8887\n",
            "Epoch [20/100], Loss: 30997.4902\n",
            "Epoch [20/100], Loss: 23386.625\n",
            "Epoch [20/100], Loss: 27897.9863\n",
            "Epoch [20/100], Loss: 37978.4688\n",
            "Epoch [20/100], Loss: 29129.3945\n",
            "Epoch [20/100], Loss: 71291.375\n",
            "Epoch [30/100], Loss: 28124.5\n",
            "Epoch [30/100], Loss: 31373.875\n",
            "Epoch [30/100], Loss: 28487.0137\n",
            "Epoch [30/100], Loss: 26272.9277\n",
            "Epoch [30/100], Loss: 38156.8242\n",
            "Epoch [30/100], Loss: 25043.9727\n",
            "Epoch [30/100], Loss: 29609.5273\n",
            "Epoch [30/100], Loss: 42630.3984\n",
            "Epoch [40/100], Loss: 28806.9238\n",
            "Epoch [40/100], Loss: 30599.252\n",
            "Epoch [40/100], Loss: 26111.6543\n",
            "Epoch [40/100], Loss: 29301.1074\n",
            "Epoch [40/100], Loss: 32741.3398\n",
            "Epoch [40/100], Loss: 34515.7656\n",
            "Epoch [40/100], Loss: 25234.457\n",
            "Epoch [40/100], Loss: 35754.5977\n",
            "Epoch [50/100], Loss: 32665.7949\n",
            "Epoch [50/100], Loss: 31248.6387\n",
            "Epoch [50/100], Loss: 35311.7891\n",
            "Epoch [50/100], Loss: 28006.4219\n",
            "Epoch [50/100], Loss: 25420.5898\n",
            "Epoch [50/100], Loss: 24193.3672\n",
            "Epoch [50/100], Loss: 30713.4199\n",
            "Epoch [50/100], Loss: 28744.1465\n",
            "Epoch [60/100], Loss: 31592.2949\n",
            "Epoch [60/100], Loss: 28517.7598\n",
            "Epoch [60/100], Loss: 29304.4473\n",
            "Epoch [60/100], Loss: 24869.1211\n",
            "Epoch [60/100], Loss: 35096.7969\n",
            "Epoch [60/100], Loss: 32558.4102\n",
            "Epoch [60/100], Loss: 24969.7676\n",
            "Epoch [60/100], Loss: 36708.375\n",
            "Epoch [70/100], Loss: 32483.2598\n",
            "Epoch [70/100], Loss: 25987.1621\n",
            "Epoch [70/100], Loss: 32671.5371\n",
            "Epoch [70/100], Loss: 22554.7949\n",
            "Epoch [70/100], Loss: 27220.3516\n",
            "Epoch [70/100], Loss: 34464.4531\n",
            "Epoch [70/100], Loss: 31971.3809\n",
            "Epoch [70/100], Loss: 26371.6211\n",
            "Epoch [80/100], Loss: 22310.1953\n",
            "Epoch [80/100], Loss: 28455.9941\n",
            "Epoch [80/100], Loss: 29856.0117\n",
            "Epoch [80/100], Loss: 31526.8359\n",
            "Epoch [80/100], Loss: 35306.8711\n",
            "Epoch [80/100], Loss: 34209.8281\n",
            "Epoch [80/100], Loss: 26434.1953\n",
            "Epoch [80/100], Loss: 11066.2832\n",
            "Epoch [90/100], Loss: 34080.0195\n",
            "Epoch [90/100], Loss: 24077.2051\n",
            "Epoch [90/100], Loss: 27424.7734\n",
            "Epoch [90/100], Loss: 25976.0645\n",
            "Epoch [90/100], Loss: 29557.7715\n",
            "Epoch [90/100], Loss: 36184.3242\n",
            "Epoch [90/100], Loss: 30338.5137\n",
            "Epoch [90/100], Loss: 15922.8105\n",
            "Epoch [100/100], Loss: 26825.2441\n",
            "Epoch [100/100], Loss: 31517.4668\n",
            "Epoch [100/100], Loss: 30123.9238\n",
            "Epoch [100/100], Loss: 25675.8398\n",
            "Epoch [100/100], Loss: 30207.7344\n",
            "Epoch [100/100], Loss: 30442.5859\n",
            "Epoch [100/100], Loss: 31101.6738\n",
            "Epoch [100/100], Loss: 42165.7773\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can see loss is significantly lower at the end than it was at the start. However, it is also bouncing around a little still which suggests the model needs more training (100 epochs is not a lot in deep learning terms). However, let's evaluate as before:"
      ],
      "metadata": {
        "id": "E72ZTKSqAODE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation (example)\n",
        "model.eval() # testing mode\n",
        "mse_values = [] # collect the MSE scores\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # predict the test data\n",
        "\n",
        "        # Calculate Mean Squared Error\n",
        "        mse = criterion(outputs, targets) # calcualte mse for the batch\n",
        "        mse_values.append(mse.item()) # add to the list of MSE values\n",
        "\n",
        "# Calculate and print the average MSE\n",
        "avg_mse = np.mean(mse_values)\n",
        "print(f\"Average MSE on test set: {avg_mse}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RbuAH6p8A-Vh",
        "outputId": "eec7b5be-ffad-4234-e60f-c04df6aac0b9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average MSE on test set: 26004.1181640625\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "MSE looks expected given training (no obvious sign of overfitting). However, we probably can get better results with tuning and more epochs.\n",
        "\n",
        "Let's run the loop again a little differently to collect the predicted values (y_hat) and actuals (y) and add them to a dataset for comparions:"
      ],
      "metadata": {
        "id": "HQ26bA08Up12"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluation\n",
        "model.eval()\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Create DataFrame\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "results_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "8AYsDDSLUp_u",
        "outputId": "45112213-500f-495c-b03a-c2b988e6807c"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "    Predicted  Actual\n",
              "0    0.622101   219.0\n",
              "1    0.622101    70.0\n",
              "2    0.622101   202.0\n",
              "3    0.622101   230.0\n",
              "4    0.622101   111.0\n",
              "..        ...     ...\n",
              "84   0.622101   153.0\n",
              "85   0.622101    98.0\n",
              "86   0.622101    37.0\n",
              "87   0.622101    63.0\n",
              "88   0.622101   184.0\n",
              "\n",
              "[89 rows x 2 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f45c6b3f-83f5-422d-9182-e8a19f2fc8fb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Predicted</th>\n",
              "      <th>Actual</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>219.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>70.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>202.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>230.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>111.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>84</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>153.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>85</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>98.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>86</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>37.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>87</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>63.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>88</th>\n",
              "      <td>0.622101</td>\n",
              "      <td>184.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>89 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f45c6b3f-83f5-422d-9182-e8a19f2fc8fb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f45c6b3f-83f5-422d-9182-e8a19f2fc8fb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f45c6b3f-83f5-422d-9182-e8a19f2fc8fb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "  <div id=\"id_992efb71-7188-478a-8c20-6ddc211f67e9\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('results_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_992efb71-7188-478a-8c20-6ddc211f67e9 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('results_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "results_df",
              "summary": "{\n  \"name\": \"results_df\",\n  \"rows\": 89,\n  \"fields\": [\n    {\n      \"column\": \"Predicted\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0.6221006512641907\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Actual\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 75,\n        \"samples\": [\n          111.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Side-by-side, they don't look great. Can you improve them?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #1\n",
        "Try increasing the number of epochs to 1,000 (when the model is fairly well trained then the results printed for each 10x epochs will be fairly stable and not change much). Does this give better results?\n",
        "\n",
        "<br><br>\n",
        "\n",
        "## EXERCISE #2 (optional)\n",
        "Try experimenting with the architecture (number of neurons and/or number of layers). Can we reach an optimal architecture?"
      ],
      "metadata": {
        "id": "LDcM98lHbgP8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Re-defining the model class for self-containment in this cell\n",
        "class DiabetesModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DiabetesModel, self).__init__()\n",
        "        self.layers = nn.Sequential(\n",
        "            nn.Linear(10, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5, 5),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(5, 1)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.layers(x)\n",
        "\n",
        "# Initialize the model, loss function, and optimizer\n",
        "# Re-initialize to ensure a fresh training start\n",
        "model = DiabetesModel()\n",
        "criterion = nn.MSELoss()\n",
        "optimiser = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Determine device for training (GPU if available, else CPU)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "# Set the new number of epochs as requested\n",
        "epochs = 1000\n",
        "\n",
        "print(f\"\\nStarting training with {epochs} epochs...\")\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in train_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        model.train() # Put the model in training mode\n",
        "        optimiser.zero_grad() # Reset the gradients\n",
        "        outputs = model(inputs) # Forward pass\n",
        "        loss = criterion(outputs, targets) # Calculate loss\n",
        "        loss.backward() # Backpropagate the loss\n",
        "        optimiser.step() # Update model parameters\n",
        "\n",
        "    # Print loss every 100 epochs to track progress\n",
        "    if (epoch + 1) % 100 == 0:\n",
        "        print(f'Epoch [{epoch+1}/{epochs}], Loss: {round(loss.item(), 4)}')\n",
        "\n",
        "print(\"\\nTraining complete. Evaluating model...\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval() # Put the model in evaluation mode\n",
        "new_mse_values = [] # Collect MSE scores for the new training run\n",
        "predictions = []\n",
        "actuals = []\n",
        "\n",
        "with torch.no_grad(): # Disable gradient calculation for evaluation\n",
        "    for inputs, targets in test_loader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs) # Make predictions on the test data\n",
        "\n",
        "        # Calculate Mean Squared Error for the batch\n",
        "        mse = criterion(outputs, targets)\n",
        "        new_mse_values.append(mse.item())\n",
        "\n",
        "        # Store predictions and actuals for DataFrame\n",
        "        predictions.extend(outputs.cpu().numpy())\n",
        "        actuals.extend(targets.cpu().numpy())\n",
        "\n",
        "# Calculate and print the average MSE on the test set\n",
        "new_avg_mse = np.mean(new_mse_values)\n",
        "print(f\"Average MSE on test set after {epochs} epochs: {new_avg_mse}\")\n",
        "\n",
        "# Create DataFrame for predictions vs. actuals\n",
        "results_df = pd.DataFrame({'Predicted': np.array(predictions).flatten(), 'Actual': np.array(actuals).flatten()})\n",
        "print(\"\\nPredictions vs. Actuals (first 5 rows):\")\n",
        "print(results_df.head())\n",
        "\n",
        "# Comment on the results\n",
        "previous_avg_mse = 26004.1181640625 # From the previous execution\n",
        "\n",
        "print(\"\\n--- Commentary on Results ---\")\n",
        "print(f\"Previous Average MSE (100 epochs): {previous_avg_mse}\")\n",
        "print(f\"New Average MSE ({epochs} epochs): {new_avg_mse}\")\n",
        "\n",
        "\n",
        "# Display the full results_df for inspection\n",
        "print(\"\\nFull Results DataFrame:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zKdxBLDyFiNr",
        "outputId": "aa77dd4f-9ad8-4ef3-912e-0b5b46f0da63"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting training with 1000 epochs...\n",
            "Epoch [100/1000], Loss: 28960.623\n",
            "Epoch [200/1000], Loss: 1135.7103\n",
            "Epoch [300/1000], Loss: 4325.0146\n",
            "Epoch [400/1000], Loss: 3561.2021\n",
            "Epoch [500/1000], Loss: 6078.603\n",
            "Epoch [600/1000], Loss: 6641.3613\n",
            "Epoch [700/1000], Loss: 3515.5886\n",
            "Epoch [800/1000], Loss: 3610.2371\n",
            "Epoch [900/1000], Loss: 1408.5449\n",
            "Epoch [1000/1000], Loss: 1799.7952\n",
            "\n",
            "Training complete. Evaluating model...\n",
            "Average MSE on test set after 1000 epochs: 2871.0079345703125\n",
            "\n",
            "Predictions vs. Actuals (first 5 rows):\n",
            "    Predicted  Actual\n",
            "0  149.570999   219.0\n",
            "1  172.765121    70.0\n",
            "2  143.600800   202.0\n",
            "3  290.782532   230.0\n",
            "4  131.129929   111.0\n",
            "\n",
            "--- Commentary on Results ---\n",
            "Previous Average MSE (100 epochs): 26004.1181640625\n",
            "New Average MSE (1000 epochs): 2871.0079345703125\n",
            "\n",
            "Full Results DataFrame:\n",
            "     Predicted  Actual\n",
            "0   149.570999   219.0\n",
            "1   172.765121    70.0\n",
            "2   143.600800   202.0\n",
            "3   290.782532   230.0\n",
            "4   131.129929   111.0\n",
            "..         ...     ...\n",
            "84  117.054016   153.0\n",
            "85   89.436668    98.0\n",
            "86   74.886353    37.0\n",
            "87   67.626671    63.0\n",
            "88  149.209885   184.0\n",
            "\n",
            "[89 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we can see from the higher average MSE, training with 1000 epochs led to better results"
      ],
      "metadata": {
        "id": "EPy7YmwIGeAR"
      }
    }
  ]
}